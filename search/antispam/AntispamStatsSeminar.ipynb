{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "# My files\n",
    "from decorators import *\n",
    "from htmlparser import HtmlInfo, html2info_parser, html2info_bs_visible\n",
    "from featureextractor import Features, calc_features, easy_tokenizer, pymorphy_tokenizer\n",
    "from tokens import read_words, write_words, get_most_frequent_words, get_word2idx\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "\n",
    "def load_csv(input_file_name, calc_features_f):    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in tqdm(enumerate(input_file)):\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)          \n",
    "            yield DocItem(url_id, mark, url, html_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 0 ns, total: 19 µs\n",
      "Wall time: 24.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_DIR = '/Users/zerogerc/Documents/datasets/antispam'\n",
    "FILE_TRAIN = os.path.join(DATA_DIR, 'kaggle_train_data_tab.csv.gz')\n",
    "FILE_TEST = os.path.join(DATA_DIR, 'kaggle_test_data_tab.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7044it [00:05, 1342.86it/s]\n"
     ]
    }
   ],
   "source": [
    "train_docs = list(load_csv(FILE_TRAIN, calc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16039it [00:12, 1269.78it/s]\n"
     ]
    }
   ],
   "source": [
    "test_docs = list(load_csv(FILE_TEST, calc_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        m1 = CountVectorizer()\n",
    "        m2 = TfidfTransformer()\n",
    "        m3 = SGDClassifier()\n",
    "\n",
    "        self.model = Pipeline([('count', m1), ('tfidf', m2), ('sgd', m3)])\n",
    "\n",
    "#     def get_X(self, docs):\n",
    "#         X = [doc.features for doc in docs]\n",
    "#         X = np.array(X)\n",
    "#         X = sklearn.preprocessing.Normalizer().fit_transform(X)\n",
    "#         return X\n",
    "    \n",
    "#     def get_X(self, docs, cache):\n",
    "#         X = []\n",
    "#         for doc in docs:\n",
    "#             if not (doc.doc_id in cache):\n",
    "#                 current = np.zeros(WORDS_COUNT)\n",
    "#                 for w in doc.features.words():\n",
    "#                     if w in word2idx:\n",
    "#                         current[word2idx[w]] += 1\n",
    "#                 Xs[doc.doc_id] = current\n",
    "                    \n",
    "#             X.append(Xs[doc.doc_id])\n",
    "            \n",
    "#         X = np.array(X)\n",
    "#         X = sklearn.preprocessing.Normalizer().fit_transform(X)\n",
    "#         return X\n",
    "    \n",
    "    def get_X(self, docs):\n",
    "        X = []\n",
    "        for doc in docs:\n",
    "            X.append(doc.features)\n",
    "#             X.append(' '.join(filter(lambda x : len(x) > 2, doc.features.words())))\n",
    "        return X\n",
    "    \n",
    "    def get_Y(self, docs):\n",
    "        Y = [1 if doc.is_spam else 0 for doc in train_docs]\n",
    "        return np.array(Y)                 \n",
    "    \n",
    "    def predict_all(self, docs):\n",
    "        X = self.get_X(docs)\n",
    "        \n",
    "        predicted = self.model.predict(X)\n",
    "        \n",
    "        res = []\n",
    "        for i, doc in enumerate(docs):\n",
    "            res.append((doc.doc_id, predicted[i]))\n",
    "        return res\n",
    "    \n",
    "    def train(self, docs):\n",
    "        X = self.get_X(docs)\n",
    "        Y = self.get_Y(docs)\n",
    "        self.model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение и запись результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "classifier.train(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url = 'mail.ru'\n",
    "test_html = '<title>Mail</title> <body>mail mail mail</body>'\n",
    "classifier.predict_all([DocItem(0, 0, test_url, test_html)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('my_submission.csv' , 'wb') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    for item in classifier.predict_all(test_docs):\n",
    "        writer.writerow([item[0], item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
